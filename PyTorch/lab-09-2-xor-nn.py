# Lab 9 XOR
import torch
from torch.autograd import Variable
import numpy as np

torch.manual_seed(777)  # for reproducibility

x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)
y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)

X = Variable(torch.from_numpy(x_data))
Y = Variable(torch.from_numpy(y_data))

linear1 = torch.nn.Linear(2,2,bias=True)
linear2 = torch.nn.Linear(2,1,bias=True)
sigmoid = torch.nn.Sigmoid()
model = torch.nn.Sequential(linear1,sigmoid,linear2,sigmoid)

optimizer = torch.optim.SGD(model.parameters(), lr=0.1)

for step in range(10001):
    optimizer.zero_grad()
    hypothesis = model(X)
    # cost/loss function
    cost = -(Y*torch.log(hypothesis) + (1-Y)*torch.log(1-hypothesis)).mean()
    cost.backward()
    optimizer.step()
    
    if step % 100 == 0:
        print(step, cost.data.numpy())
        
# Accuracy computation
# True if hypothesis>0.5 else False
predicted = (model(X).data > 0.5).float()
accuracy = (predicted == Y.data).float().mean()
print("\nHypothesis: ", hypothesis.data.numpy(), "\nCorrect: ", predicted.numpy(), "\nAccuracy: ", accuracy)
